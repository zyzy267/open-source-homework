"""
GitHub Issueå…ƒæ•°æ®æ”¶é›†è„šæœ¬
å…·æœ‰é”™è¯¯å¤„ç†ã€æ¢å¤èƒ½åŠ›å’Œç½‘ç»œæ£€æµ‹åŠŸèƒ½
"""

import requests
import json
import time
import os
import sys
import argparse
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('github_collector.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class GitHubIssueCollector:
    def __init__(self, token: Optional[str] = None):
        """
        åˆå§‹åŒ–GitHub Issueæ”¶é›†å™¨

        Args:
            token: GitHubä¸ªäººè®¿é—®ä»¤ç‰Œï¼Œå¦‚æœä¸ºNoneåˆ™æç¤ºç”¨æˆ·è¾“å…¥
        """
        self.token = token or self._get_token_from_user()
        self.base_url = "https://api.github.com"
        self.session = self._create_session()
        self.max_retries = 5
        self.retry_delay = 5  # åˆå§‹é‡è¯•å»¶è¿Ÿç§’æ•°
        self.max_wait_time = 300  # æœ€å¤§ç­‰å¾…æ—¶é—´5åˆ†é’Ÿ
        self.collected_data = []

    def _get_token_from_user(self) -> str:
        """ä»ç”¨æˆ·è¾“å…¥è·å–GitHub Token"""
        print("\n" + "=" * 60)
        print("GitHub Issueå…ƒæ•°æ®æ”¶é›†å™¨")
        print("=" * 60)

        token = input("è¯·è¾“å…¥GitHubä¸ªäººè®¿é—®ä»¤ç‰Œ: ").strip()
        if not token:
            logger.error("å¿…é¡»æä¾›GitHub Token")
            sys.exit(1)

        # éªŒè¯tokenæ ¼å¼
        if not token.startswith(("ghp_", "github_pat_")):
            logger.warning("Tokenæ ¼å¼å¯èƒ½ä¸æ­£ç¡®ï¼Œæ ‡å‡†æ ¼å¼ä»¥'ghp_'æˆ–'github_pat_'å¼€å¤´")

        return token

    def _create_session(self) -> requests.Session:
        """åˆ›å»ºHTTPä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            "Authorization": f"Bearer {self.token}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "GitHub-Issue-Collector/1.0"
        })
        session.timeout = 30
        return session

    def _make_request_with_retry(self, url: str, params: Dict = None,
                                 method: str = "GET") -> Optional[requests.Response]:
        """
        å¸¦æœ‰é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚

        Args:
            url: è¯·æ±‚URL
            params: æŸ¥è¯¢å‚æ•°
            method: HTTPæ–¹æ³•

        Returns:
            å“åº”å¯¹è±¡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"è¯·æ±‚ {url} (å°è¯• {attempt + 1}/{self.max_retries})")

                if method.upper() == "GET":
                    response = self.session.get(url, params=params, timeout=30)
                elif method.upper() == "HEAD":
                    response = self.session.head(url, params=params, timeout=30)
                else:
                    response = self.session.get(url, params=params, timeout=30)

                # å¤„ç†æˆåŠŸå“åº”
                if response.status_code in [200, 201, 202]:
                    return response

                # å¤„ç†ç‰¹æ®ŠçŠ¶æ€ç 
                elif response.status_code == 401:
                    logger.error("è®¤è¯å¤±è´¥ï¼šæ— æ•ˆçš„GitHub Token")
                    print(" è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥æ‚¨çš„GitHub Tokenæ˜¯å¦æ­£ç¡®")
                    return None

                elif response.status_code == 403:
                    # å¤„ç†é€Ÿç‡é™åˆ¶
                    remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
                    reset_time = int(response.headers.get('X-RateLimit-Reset', 0))

                    if remaining == 0:
                        wait_time = max(reset_time - time.time(), 0) + 5
                        logger.warning(f"è¾¾åˆ°APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.0f} ç§’")
                        print(f" è¾¾åˆ°APIé™åˆ¶ï¼Œç­‰å¾… {wait_time:.0f} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"è®¿é—®è¢«æ‹’ç»: {response.status_code}")
                        print(f" è®¿é—®è¢«æ‹’ç» (HTTP {response.status_code})")
                        return None

                elif response.status_code == 404:
                    logger.error(f"èµ„æºä¸å­˜åœ¨: {url}")
                    return None

                elif response.status_code in [500, 502, 503, 504]:
                    logger.warning(f"æœåŠ¡å™¨é”™è¯¯: {response.status_code}")
                    wait_time = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    print(f" æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue

                else:
                    logger.error(f"HTTPé”™è¯¯ {response.status_code}: {response.text[:200]}")
                    if attempt < self.max_retries - 1:
                        wait_time = self.retry_delay * (2 ** attempt)
                        time.sleep(wait_time)
                        continue
                    return None

            except requests.exceptions.Timeout:
                logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (2 ** attempt)
                    print(f" è¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                continue

            except requests.exceptions.ConnectionError as e:
                logger.warning(f"è¿æ¥é”™è¯¯: {e} (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (2 ** attempt)
                    print(f" ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                continue

            except requests.exceptions.RequestException as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (2 ** attempt)
                    time.sleep(wait_time)
                continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"æ‰€æœ‰ {self.max_retries} æ¬¡é‡è¯•éƒ½å¤±è´¥")
        return None

    def check_repository_exists(self, repo_name: str) -> Tuple[bool, Dict]:
        """
        æ£€æŸ¥GitHubä»“åº“æ˜¯å¦å­˜åœ¨

        Args:
            repo_name: ä»“åº“åç§°ï¼Œæ ¼å¼: owner/repo

        Returns:
            (æ˜¯å¦å­˜åœ¨, ä»“åº“ä¿¡æ¯)
        """
        if '/' not in repo_name or repo_name.count('/') != 1:
            logger.error(f"ä»“åº“åç§°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 'owner/repo' æ ¼å¼: {repo_name}")
            return False, {}

        url = f"{self.base_url}/repos/{repo_name}"
        logger.info(f"æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨: {repo_name}")

        response = self._make_request_with_retry(url, method="GET")

        if response and response.status_code == 200:
            repo_info = response.json()
            logger.info(f"ä»“åº“æ‰¾åˆ°: {repo_info.get('full_name')}")
            logger.info(f"æè¿°: {repo_info.get('description', 'æ— æè¿°')}")
            logger.info(f"æ˜Ÿæ ‡æ•°: {repo_info.get('stargazers_count')}")
            logger.info(f"æœ€åæ›´æ–°: {repo_info.get('updated_at')}")
            return True, repo_info
        else:
            logger.error(f"ä»“åº“ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {repo_name}")
            return False, {}

    def get_issues_for_repository(self, repo_name: str, state: str = "all",
                                  per_page: int = 100, max_pages: int = 100) -> List[Dict]:
        """
        è·å–æŒ‡å®šä»“åº“çš„æ‰€æœ‰Issue

        Args:
            repo_name: ä»“åº“åç§°
            state: IssueçŠ¶æ€ (all, open, closed)
            per_page: æ¯é¡µæ•°é‡
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶

        Returns:
            Issueåˆ—è¡¨
        """
        issues = []
        page = 1
        has_more_pages = True

        # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        checkpoint_file = f"checkpoint_{repo_name.replace('/', '_')}.json"

        # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        checkpoint = self._load_checkpoint(checkpoint_file)
        if checkpoint:
            page = checkpoint.get('page', 1)
            issues = checkpoint.get('issues', [])
            logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {page} é¡µï¼Œå·²æ”¶é›† {len(issues)} ä¸ªIssue")

        # è¿›åº¦è·Ÿè¸ª
        start_time = time.time()
        last_save_time = time.time()
        save_interval = 60  # æ¯60ç§’ä¿å­˜ä¸€æ¬¡

        try:
            while has_more_pages and page <= max_pages:
                logger.info(f"è·å– {repo_name} çš„Issueæ•°æ®ï¼Œç¬¬ {page} é¡µ...")

                url = f"{self.base_url}/repos/{repo_name}/issues"
                params = {
                    "state": state,
                    "per_page": per_page,
                    "page": page,
                    "direction": "asc",  # æŒ‰åˆ›å»ºæ—¶é—´å‡åº
                    "filter": "all"  # åŒ…æ‹¬æ‰€æœ‰Issueå’ŒPR
                }

                response = self._make_request_with_retry(url, params)

                if not response:
                    logger.error(f"ç¬¬ {page} é¡µè·å–å¤±è´¥")
                    if page > 1:
                        logger.info("ä¿å­˜å·²æ”¶é›†çš„æ•°æ®...")
                        self._save_checkpoint(checkpoint_file, page, issues)
                    break

                batch_issues = response.json()

                if not batch_issues:
                    logger.info(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œå®Œæˆæ”¶é›†")
                    has_more_pages = False
                    break

                # è§£æIssueæ•°æ®
                parsed_issues = []
                seen_issue_numbers = {issue.get('number') for issue in issues}

                for issue in batch_issues:
                    issue_number = issue.get("number")

                    # è·³è¿‡å·²æ”¶é›†çš„Issue
                    if issue_number in seen_issue_numbers:
                        continue

                    issue_data = self._parse_issue_data(issue)
                    parsed_issues.append(issue_data)
                    seen_issue_numbers.add(issue_number)

                issues.extend(parsed_issues)
                logger.info(f"ç¬¬ {page} é¡µ: æ”¶é›†åˆ° {len(parsed_issues)} ä¸ªæ–°Issueï¼Œæ€»è®¡ {len(issues)} ä¸ª")

                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                if len(batch_issues) < per_page:
                    logger.info(f"ç¬¬ {page} é¡µæ•°æ®ä¸è¶³ {per_page} æ¡ï¼Œå¯èƒ½æ˜¯æœ€åä¸€é¡µ")
                    has_more_pages = False
                else:
                    page += 1

                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                current_time = time.time()
                if current_time - last_save_time > save_interval or not has_more_pages:
                    self._save_checkpoint(checkpoint_file, page, issues)
                    last_save_time = current_time

                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)

            # æ”¶é›†å®Œæˆ
            elapsed_time = time.time() - start_time
            logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼è€—æ—¶ {elapsed_time:.1f} ç§’ï¼Œå…±æ”¶é›† {len(issues)} ä¸ªIssue")

            # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
            if os.path.exists(checkpoint_file):
                os.remove(checkpoint_file)
                logger.info(f"æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ¸…ç†: {checkpoint_file}")

            return issues

        except Exception as e:
            logger.error(f"æ”¶é›†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            logger.info("ä¿å­˜å·²æ”¶é›†çš„æ•°æ®åˆ°æ£€æŸ¥ç‚¹...")
            self._save_checkpoint(checkpoint_file, page, issues)
            raise

    def _parse_issue_data(self, issue: Dict) -> Dict[str, Any]:
        """è§£æIssueæ•°æ®"""
        # å®‰å…¨è·å–labels
        labels = []
        if issue.get("labels"):
            labels = [
                label.get("name", "")
                for label in issue["labels"]
                if label and isinstance(label, dict) and label.get("name")
            ]

        # å®‰å…¨è·å–userä¿¡æ¯
        user_login = None
        if issue.get("user") and isinstance(issue["user"], dict):
            user_login = issue["user"].get("login")

        return {
            "id": issue.get("id"),
            "number": issue.get("number"),
            "title": issue.get("title", ""),
            "created_at": issue.get("created_at"),
            "updated_at": issue.get("updated_at"),
            "closed_at": issue.get("closed_at"),
            "state": issue.get("state", "").lower(),
            "comments_count": issue.get("comments", 0),
            "labels": labels,
            "user_login": user_login,
            "is_pull_request": "pull_request" in issue,
            "body_length": len(issue.get("body") or ""),
            "html_url": issue.get("html_url", ""),
            "locked": issue.get("locked", False),
            "assignee_count": len(issue.get("assignees", []))
        }

    def _save_checkpoint(self, checkpoint_file: str, page: int, issues: List[Dict]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = {
                "timestamp": datetime.now().isoformat(),
                "page": page,
                "total_issues": len(issues),
                "issues": issues
            }

            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2, default=str)

            logger.debug(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_file} (ç¬¬ {page} é¡µ)")
        except Exception as e:
            logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def _load_checkpoint(self, checkpoint_file: str) -> Optional[Dict]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_file):
            return None

        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_file}")
            logger.info(f"  æ—¶é—´: {data.get('timestamp')}")
            logger.info(f"  é¡µæ•°: {data.get('page')}")
            logger.info(f"  Issueæ•°: {data.get('total_issues')}")
            return data
        except Exception as e:
            logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

    def save_issues_to_file(self, issues: List[Dict], repo_name: str,
                            output_dir: str = "data"):
        """
        ä¿å­˜Issueæ•°æ®åˆ°æ–‡ä»¶

        Args:
            issues: Issueåˆ—è¡¨
            repo_name: ä»“åº“åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)

        # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
        safe_repo_name = repo_name.replace('/', '_').replace('\\', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSONæ–‡ä»¶
        json_file = os.path.join(output_dir, f"{safe_repo_name}_issues_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(issues, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"JSONæ•°æ®å·²ä¿å­˜: {json_file}")

        # CSVæ–‡ä»¶
        if issues:
            csv_file = os.path.join(output_dir, f"{safe_repo_name}_issues_{timestamp}.csv")
            import csv

            fieldnames = issues[0].keys()
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for issue in issues:
                    row = issue.copy()
                    if isinstance(row["labels"], list):
                        row["labels"] = ";".join(row["labels"])
                    writer.writerow(row)
            logger.info(f"CSVæ•°æ®å·²ä¿å­˜: {csv_file}")

        return json_file

    def print_statistics(self, issues: List[Dict], repo_name: str):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not issues:
            print("æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
            return

        print("\n" + "=" * 60)
        print(f" {repo_name} é¡¹ç›®Issueç»Ÿè®¡")
        print("=" * 60)

        total = len(issues)
        open_issues = len([i for i in issues if i["state"] == "open"])
        closed_issues = len([i for i in issues if i["state"] == "closed"])
        pr_count = len([i for i in issues if i["is_pull_request"]])

        print(f"æ€»è®¡ Issue/PR æ•°é‡: {total:,}")
        print(f"Open Issues: {open_issues:,} ({open_issues / total * 100:.1f}%)")
        print(f"Closed Issues: {closed_issues:,} ({closed_issues / total * 100:.1f}%)")
        print(f"Pull Requests: {pr_count:,} ({pr_count / total * 100:.1f}%)")

        # æ—¶é—´èŒƒå›´
        dates = [i["created_at"] for i in issues if i["created_at"]]
        if dates:
            dates.sort()
            print(f"æ—¶é—´èŒƒå›´: {dates[0]} åˆ° {dates[-1]}")

        # æ ‡ç­¾ç»Ÿè®¡
        all_labels = []
        for issue in issues:
            if issue.get("labels"):
                all_labels.extend(issue["labels"])

        if all_labels:
            from collections import Counter
            label_counts = Counter(all_labels)
            print(f"æ ‡ç­¾æ€»æ•°: {len(all_labels):,} (å»é‡: {len(label_counts):,})")

        print("=" * 60)


def check_network_connectivity() -> bool:
    """
    æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§

    Returns:
        ç½‘ç»œæ˜¯å¦å¯ç”¨
    """
    test_urls = [
        "https://api.github.com",
        "https://www.google.com",
        "https://www.baidu.com"
    ]

    for url in test_urls:
        try:
            response = requests.head(url, timeout=10)
            if response.status_code < 500:
                logger.info(f"ç½‘ç»œè¿æ¥æ­£å¸¸: {url}")
                return True
        except Exception as e:
            logger.debug(f"ç½‘ç»œæ£€æŸ¥å¤±è´¥ {url}: {e}")
            continue

    return False


def handle_critical_error(collector: GitHubIssueCollector, error: Exception):
    """
    å¤„ç†ä¸¥é‡é”™è¯¯

    Args:
        collector: æ”¶é›†å™¨å®ä¾‹
        error: å¼‚å¸¸å¯¹è±¡
    """
    print("\n" + "=" * 60)
    print(" å‘ç”Ÿä¸å¯æ¢å¤é”™è¯¯")
    print("=" * 60)
    print(f"é”™è¯¯ç±»å‹: {type(error).__name__}")
    print(f"é”™è¯¯ä¿¡æ¯: {str(error)[:200]}")

    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    print("\næ­£åœ¨æ£€æµ‹æ‚¨çš„ç½‘ç»œç¯å¢ƒ...")
    for i in range(3):  # å°è¯•3æ¬¡ç½‘ç»œæ£€æµ‹
        print(f"ç½‘ç»œæ£€æµ‹å°è¯• {i + 1}/3...")
        if check_network_connectivity():
            print("ç½‘ç»œè¿æ¥æ­£å¸¸")
            return

        wait_time = 60  # æ¯æ¬¡ç­‰å¾…60ç§’
        print(f" ç½‘ç»œæ£€æµ‹å¤±è´¥ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
        time.sleep(wait_time)

    print("ç½‘ç»œæ£€æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥")
    print("ç¨‹åºå°†åœ¨é•¿æ—¶é—´ç­‰å¾…åé€€å‡º...")
    print("=" * 60)

    # é•¿æ—¶é—´ç­‰å¾…
    for remaining in range(300, 0, -60):  # 5åˆ†é’Ÿå€’è®¡æ—¶
        if remaining > 60:
            print(f"â³ ç­‰å¾… {remaining // 60} åˆ†é’Ÿ {remaining % 60} ç§’åé€€å‡º...")
        else:
            print(f"â³ ç­‰å¾… {remaining} ç§’åé€€å‡º...")
        time.sleep(60)

    print("ç¨‹åºé€€å‡º")
    sys.exit(1)


def get_repository_name() -> str:
    """è·å–ç”¨æˆ·è¾“å…¥çš„ä»“åº“åç§°"""
    while True:
        print("\n" + "=" * 60)
        print("ğŸ“¦ GitHubä»“åº“Issueæ”¶é›†å™¨")
        print("=" * 60)
        print("æ”¯æŒçš„ä»“åº“æ ¼å¼:")
        print("  - owner/repository  (ä¾‹å¦‚: encode/httpx)")
        print("  - psf/requests")
        print("  - facebook/react")
        print("=" * 60)

        repo_name = input("è¯·è¾“å…¥GitHubä»“åº“åç§° (owner/repo): ").strip()

        if not repo_name:
            print("ä»“åº“åç§°ä¸èƒ½ä¸ºç©º")
            continue

        if '/' not in repo_name or repo_name.count('/') != 1:
            print("æ ¼å¼é”™è¯¯ï¼è¯·ä½¿ç”¨ 'owner/repository' æ ¼å¼")
            continue

        return repo_name


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='GitHub Issueå…ƒæ•°æ®æ”¶é›†å™¨')
        parser.add_argument('--token', help='GitHubè®¿é—®ä»¤ç‰Œ')
        parser.add_argument('--repo', help='GitHubä»“åº“åç§° (owner/repo)')
        parser.add_argument('--state', default='all', choices=['all', 'open', 'closed'],
                            help='IssueçŠ¶æ€')
        parser.add_argument('--max-pages', type=int, default=100,
                            help='æœ€å¤§é¡µæ•°é™åˆ¶')
        parser.add_argument('--output-dir', default='data',
                            help='è¾“å‡ºç›®å½•')

        args = parser.parse_args()

        # åˆ›å»ºæ”¶é›†å™¨
        collector = GitHubIssueCollector(token=args.token)

        # è·å–ä»“åº“åç§°
        repo_name = args.repo or get_repository_name()

        # æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
        exists, repo_info = collector.check_repository_exists(repo_name)
        if not exists:
            print(f"\n ä»“åº“ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {repo_name}")
            print("è¯·æ£€æŸ¥:")
            print("  1. ä»“åº“åç§°æ˜¯å¦æ­£ç¡®")
            print("  2. ä»“åº“æ˜¯å¦ä¸ºå…¬å¼€ä»“åº“")
            print("  3. æ‚¨çš„GitHub Tokenæ˜¯å¦æœ‰è®¿é—®æƒé™")
            return

        print(f"\n æ‰¾åˆ°ä»“åº“: {repo_name}")
        print(f"  æè¿°: {repo_info.get('description', 'æ— æè¿°')}")
        print(f"  æ˜Ÿæ ‡æ•°: {repo_info.get('stargazers_count'):,}")
        print(f"  æœ€åæ›´æ–°: {repo_info.get('updated_at')}")

        # ç¡®è®¤æ˜¯å¦ç»§ç»­
        confirm = input("\næ˜¯å¦å¼€å§‹æ”¶é›†Issueæ•°æ®ï¼Ÿ(y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("æ“ä½œå–æ¶ˆ")
            return

        # å¼€å§‹æ”¶é›†
        print(f"\n å¼€å§‹æ”¶é›† {repo_name} çš„Issueæ•°æ®...")
        print("æ³¨æ„: è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ï¼Œå–å†³äºä»“åº“å¤§å°")
        print("=" * 60)

        try:
            issues = collector.get_issues_for_repository(
                repo_name=repo_name,
                state=args.state,
                per_page=100,
                max_pages=args.max_pages
            )

            if issues:
                # ä¿å­˜æ•°æ®
                json_file = collector.save_issues_to_file(
                    issues=issues,
                    repo_name=repo_name,
                    output_dir=args.output_dir
                )

                # æ‰“å°ç»Ÿè®¡
                collector.print_statistics(issues, repo_name)

                print(f"\n æ•°æ®æ”¶é›†å®Œæˆï¼")
                print(f" æ•°æ®æ–‡ä»¶: {json_file}")
                print(f" Issueæ•°é‡: {len(issues):,}")
                print("=" * 60)
            else:
                print("\n æ²¡æœ‰æ”¶é›†åˆ°Issueæ•°æ®")
                print("å¯èƒ½çš„åŸå› :")
                print("  1. ä»“åº“æ²¡æœ‰Issue")
                print("  2. APIè®¿é—®å—é™")
                print("  3. ç½‘ç»œé—®é¢˜")

        except Exception as e:
            logger.error(f"æ”¶é›†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            handle_critical_error(collector, e)

    except KeyboardInterrupt:
        print("\n\n ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        print("å·²ä¿å­˜çš„æ•°æ®å¯ä»¥åœ¨dataç›®å½•æ‰¾åˆ°")

    except Exception as e:
        print(f"\n ç¨‹åºå‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {e}", exc_info=True)
        print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: github_collector.log")


if __name__ == "__main__":
    main()